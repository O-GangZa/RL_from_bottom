{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "강화학습 #9 실습.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RaAm6-L4KNO"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn                      # torch내부의 매서드를 정의해줘서 코드를 짧게 짤 수 있게 해준다.\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# 하이퍼 파라미터\n",
        "learning_rate = 0.0002\n",
        "gamma = 0.98\n",
        "epoch = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 메인 함수\n"
      ],
      "metadata": {
        "id": "GLrhrmaw4qgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    env = gym.make('CartPole-v1') #카트폴 환경을 env로 설정\n",
        "    pi = Policy()   # 정책 클래스를 호출해주면 __call__에 의해 forward가 자동 실행된다.\n",
        "    score = 0.0     # 리턴이 곧 score\n",
        "    print_interval = 20\n",
        "\n",
        "    for n_epi in range(epoch):\n",
        "        s = env.reset()   # 초기 state로 리셋해주고 시작\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            prob = pi(torch.from_numpy(s).float())\n",
        "            # 아마도 확률을 관측값에 따라 변동시켜주는 것, 다시말해 정책 π_θ에 해당하는 것 같다.\n",
        "            m = Categorical(prob) # 행동의 경우의 수가 적은경우 categorical을 쓴다고함\n",
        "            # 샘플이 쌓일때마다 확률이 높은 액션은 자주, 낮은 액션은 덜 뽑히게 됨, \n",
        "            a = m.sample()\n",
        "            s_prime, r, done, info = env.step(a.item())\n",
        "            pi.put_data((r, prob[a]))\n",
        "            s = s_prime\n",
        "            score += r\n",
        "\n",
        "        pi.train_net()\n",
        "\n",
        "        if n_epi%print_interval==0 and n_epi!=0: #인터벌로 나눴을 때 0이고, 첫 에피소드가 아니면\n",
        "            print(\"# of epidode :{}, avg score : {}\".format(n_epi, score/print_interval))\n",
        "            score = 0.0\n",
        "    env.close()\n"
      ],
      "metadata": {
        "id": "QCwPtWOc4jaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 정책 네트워크 클래스"
      ],
      "metadata": {
        "id": "KuwcUWtV6lhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Policy, self).__init__()\n",
        "        self.data = []\n",
        "\n",
        "        self.fc1 = nn.Linear(4, 128) #fc없는 단순 모델\n",
        "        self.fc2 = nn.Linear(128, 2)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.softmax(self.fc2(x), dim=0)\n",
        "        #아마도? 정책은 확률값으로 나타나니까 소프트맥스를 써준것 같다.\n",
        "        return x\n",
        "\n",
        "    def put_data(self, item):\n",
        "        self.data.append(item)\n",
        "\n",
        "    def train_net(self):\n",
        "        R = 0\n",
        "        self.optimizer.zero_grad() # gradient 계산후 변수가 남아있어서 일단 매번 리셋해준다고 함\n",
        "        for r, prob in self.data[::-1]:\n",
        "            R = r + gamma * R #리턴 식\n",
        "            loss = -R * torch.log(prob) # 9.2에서 손실함수, 정리 노트 참조\n",
        "            loss.backward() # 그래디언트 계산 후\n",
        "        self.optimizer.step() # 그래디언트 방향으로 lr만큼 θ를 조정\n",
        "        self.data = []"
      ],
      "metadata": {
        "id": "yY9D8Lkq4_z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiTCXbeN71ZG",
        "outputId": "85fc2857-fce3-4a1f-adfd-9f72853fbbb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of epidode :20, avg score : 29.15\n",
            "# of epidode :40, avg score : 19.85\n",
            "# of epidode :60, avg score : 23.2\n",
            "# of epidode :80, avg score : 25.1\n",
            "# of epidode :100, avg score : 23.95\n",
            "# of epidode :120, avg score : 24.8\n",
            "# of epidode :140, avg score : 28.75\n",
            "# of epidode :160, avg score : 28.35\n",
            "# of epidode :180, avg score : 27.75\n",
            "# of epidode :200, avg score : 35.35\n",
            "# of epidode :220, avg score : 35.7\n",
            "# of epidode :240, avg score : 35.15\n",
            "# of epidode :260, avg score : 39.6\n",
            "# of epidode :280, avg score : 32.65\n",
            "# of epidode :300, avg score : 35.35\n",
            "# of epidode :320, avg score : 42.35\n",
            "# of epidode :340, avg score : 50.6\n",
            "# of epidode :360, avg score : 39.45\n",
            "# of epidode :380, avg score : 39.25\n",
            "# of epidode :400, avg score : 36.2\n",
            "# of epidode :420, avg score : 62.9\n",
            "# of epidode :440, avg score : 48.85\n",
            "# of epidode :460, avg score : 52.75\n",
            "# of epidode :480, avg score : 53.9\n",
            "# of epidode :500, avg score : 48.15\n",
            "# of epidode :520, avg score : 54.7\n",
            "# of epidode :540, avg score : 61.4\n",
            "# of epidode :560, avg score : 74.7\n",
            "# of epidode :580, avg score : 51.7\n",
            "# of epidode :600, avg score : 53.35\n",
            "# of epidode :620, avg score : 59.5\n",
            "# of epidode :640, avg score : 74.05\n",
            "# of epidode :660, avg score : 77.65\n",
            "# of epidode :680, avg score : 68.85\n",
            "# of epidode :700, avg score : 81.25\n",
            "# of epidode :720, avg score : 79.3\n",
            "# of epidode :740, avg score : 64.3\n",
            "# of epidode :760, avg score : 81.45\n",
            "# of epidode :780, avg score : 83.65\n",
            "# of epidode :800, avg score : 85.75\n",
            "# of epidode :820, avg score : 103.9\n",
            "# of epidode :840, avg score : 78.2\n",
            "# of epidode :860, avg score : 102.45\n",
            "# of epidode :880, avg score : 107.45\n",
            "# of epidode :900, avg score : 87.4\n",
            "# of epidode :920, avg score : 109.5\n",
            "# of epidode :940, avg score : 128.15\n",
            "# of epidode :960, avg score : 146.65\n",
            "# of epidode :980, avg score : 159.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TD Actor-Critic 구현"
      ],
      "metadata": {
        "id": "_HeO8uDNHmoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 라이브러리 import 및 하이퍼 파라미터 정의\n"
      ],
      "metadata": {
        "id": "vxTJOC5MHyuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# 하이퍼 파라미터\n",
        "learning_rate = 0.0002\n",
        "gamma = 0.98\n",
        "epoch = 1000\n",
        "n_rollout = 10   # 10개의 상태전이를 모아서 업데이트함"
      ],
      "metadata": {
        "id": "sjGIgrb6720I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 메인 함수"
      ],
      "metadata": {
        "id": "FjAscgTOIlP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    env = gym.make('CartPole-v1')\n",
        "    model = ActorCritic()\n",
        "    print_interval = 20 #20회마다 에피소드 수랑 스코어 출력\n",
        "    score = 0\n",
        "\n",
        "    for n_epi in range(epoch):\n",
        "        done = False\n",
        "        s = env.reset()\n",
        "\n",
        "        while not done:\n",
        "            for t in range(n_rollout): #10번을 채울때까지 put_data에 데이터를 일단 모아두자\n",
        "                prob = model.pi(torch.from_numpy(s).float()) #정책 : 행동확률을 결정\n",
        "                m = Categorical(prob) #확률에 따른 행동 분포를 만들어 주는듯 (coin)대신 선택하도록\n",
        "                a = m.sample().item() #확률에 따라 액션 선택!\n",
        "                s_prime, r, done, info = env.step(a) #step에 따른 데이터를 받아서\n",
        "                model.put_data((s,a,r,s_prime,done)) # 데이터를 저장 후\n",
        "\n",
        "                s = s_prime\n",
        "                score += r\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            model.train_net() #한번에 트레인\n",
        "\n",
        "        if n_epi%print_interval==0 and n_epi!=0: \n",
        "            print(\"# 에피소드수 :{}, 평균 스코어 : {}\".format(n_epi, score/print_interval))\n",
        "            score = 0.0\n",
        "    env.close()"
      ],
      "metadata": {
        "id": "19e4ziJXIFgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 액터 크리틱 클래스\n"
      ],
      "metadata": {
        "id": "GgCznKEJLzmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.data = []\n",
        "\n",
        "        self.fc1 = nn.Linear(4, 256)\n",
        "        self.fc_pi = nn.Linear(256, 2) # π를 업데이트하는 뉴럴넷 (정책 함수 파라미터)\n",
        "        self.fc_v = nn.Linear(256, 1) # Φ를 업데이느 하는 뉴럴넷 (상태 가치함수 파라미터)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def pi(self, x, softmax_dim = 0): # pi함수 fc1 -> relu -> fcpi -> softmax ->prob (어떤 행동을 할지에 대한 확률 정책)\n",
        "        x = F.relu(self.fc1(x)) \n",
        "        x = self.fc_pi(x)\n",
        "        prob = F.softmax(x, dim=softmax_dim) # 취할수 있는 차량의 행동은 좌, 우 두개이므로 2개의 아웃풋\n",
        "        return prob\n",
        "\n",
        "    def v(self, x): # v함수 fc1->relu -> fc_v -> 하나의 결과값 (입력 s(4개 요소)에 대한 상태 가치를 리턴)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        v = self.fc_v(x)\n",
        "        return v\n",
        "\n",
        "    def put_data(self, transition): #rollout까지 transition 들을 업데이트\n",
        "        self.data.append(transition)\n",
        "\n",
        "    def make_batch(self):\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, done_lst = [], [], [], [], [] #각각의 데이터를 리스트에 넣는다\n",
        "        for transition in self.data: #data에서 batch를 꺼내서 넣어줌\n",
        "            s, a, r, s_prime, done = transition\n",
        "            s_lst.append(s)\n",
        "            a_lst.append([a]) #액션은 복수라 리스트인듯?\n",
        "            r_lst.append([r/100.0]) # 액션에따른 리턴이라 리스트 (값이 커질가능성이 있어 100으로 나눠준다고함)\n",
        "            s_prime_lst.append(s_prime)\n",
        "            done_mask = 0.0 if done else 1.0 # done여부 원핫코딩\n",
        "            done_lst.append([done_mask])\n",
        "\n",
        "        s_batch, a_batch, r_batch, s_prime_batch, done_batch = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), torch.tensor(r_lst,  dtype=torch.float), torch.tensor(s_prime_lst,  dtype=torch.float), torch.tensor(done_lst,  dtype=torch.float)\n",
        "        #각 값을 토치 텐서, float으로 변화해서 리턴함\n",
        "        self.data = []\n",
        "        return s_batch, a_batch, r_batch, s_prime_batch, done_batch\n",
        "\n",
        "    def train_net(self):\n",
        "        s, a, r, s_prime, done = self.make_batch()\n",
        "        td_target = r + gamma*self.v(s_prime)*done\n",
        "        delta = td_target - self.v(s)\n",
        "\n",
        "        pi = self.pi(s, softmax_dim=1)\n",
        "        pi_a = pi.gather(1,a) # pi에 있는 1번 축에서 a데이터만 모은다 인것 같다.\n",
        "        loss = -torch.log(pi_a)*delta.detach() + F.smooth_l1_loss(self.v(s), td_target.detach()) #detach는 미분시 상수로 취급\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.mean().backward() # 10개의 loss의 평균을 최종 loss로 정의\n",
        "        self.optimizer.step()"
      ],
      "metadata": {
        "id": "y-VZiUScLqIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FelXLAd-Owo3",
        "outputId": "7bbed358-346e-47f7-8d5a-d8246c189577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 에피소드수 :20, 평균 스코어 : 27.15\n",
            "# 에피소드수 :40, 평균 스코어 : 17.5\n",
            "# 에피소드수 :60, 평균 스코어 : 29.65\n",
            "# 에피소드수 :80, 평균 스코어 : 28.2\n",
            "# 에피소드수 :100, 평균 스코어 : 25.35\n",
            "# 에피소드수 :120, 평균 스코어 : 21.65\n",
            "# 에피소드수 :140, 평균 스코어 : 29.95\n",
            "# 에피소드수 :160, 평균 스코어 : 37.2\n",
            "# 에피소드수 :180, 평균 스코어 : 34.15\n",
            "# 에피소드수 :200, 평균 스코어 : 34.4\n",
            "# 에피소드수 :220, 평균 스코어 : 41.85\n",
            "# 에피소드수 :240, 평균 스코어 : 47.5\n",
            "# 에피소드수 :260, 평균 스코어 : 54.6\n",
            "# 에피소드수 :280, 평균 스코어 : 62.35\n",
            "# 에피소드수 :300, 평균 스코어 : 89.95\n",
            "# 에피소드수 :320, 평균 스코어 : 135.5\n",
            "# 에피소드수 :340, 평균 스코어 : 121.45\n",
            "# 에피소드수 :360, 평균 스코어 : 173.85\n",
            "# 에피소드수 :380, 평균 스코어 : 215.15\n",
            "# 에피소드수 :400, 평균 스코어 : 183.85\n",
            "# 에피소드수 :420, 평균 스코어 : 170.75\n",
            "# 에피소드수 :440, 평균 스코어 : 219.35\n",
            "# 에피소드수 :460, 평균 스코어 : 213.65\n",
            "# 에피소드수 :480, 평균 스코어 : 283.4\n",
            "# 에피소드수 :500, 평균 스코어 : 244.9\n",
            "# 에피소드수 :520, 평균 스코어 : 206.2\n",
            "# 에피소드수 :540, 평균 스코어 : 211.3\n",
            "# 에피소드수 :560, 평균 스코어 : 239.7\n",
            "# 에피소드수 :580, 평균 스코어 : 366.2\n",
            "# 에피소드수 :600, 평균 스코어 : 293.05\n",
            "# 에피소드수 :620, 평균 스코어 : 272.2\n",
            "# 에피소드수 :640, 평균 스코어 : 260.9\n",
            "# 에피소드수 :660, 평균 스코어 : 274.15\n",
            "# 에피소드수 :680, 평균 스코어 : 351.9\n",
            "# 에피소드수 :700, 평균 스코어 : 424.0\n",
            "# 에피소드수 :720, 평균 스코어 : 329.05\n",
            "# 에피소드수 :740, 평균 스코어 : 304.8\n",
            "# 에피소드수 :760, 평균 스코어 : 366.1\n",
            "# 에피소드수 :780, 평균 스코어 : 211.25\n",
            "# 에피소드수 :800, 평균 스코어 : 365.0\n",
            "# 에피소드수 :820, 평균 스코어 : 353.65\n",
            "# 에피소드수 :840, 평균 스코어 : 327.75\n",
            "# 에피소드수 :860, 평균 스코어 : 355.1\n",
            "# 에피소드수 :880, 평균 스코어 : 429.75\n",
            "# 에피소드수 :900, 평균 스코어 : 358.8\n",
            "# 에피소드수 :920, 평균 스코어 : 343.55\n",
            "# 에피소드수 :940, 평균 스코어 : 402.25\n",
            "# 에피소드수 :960, 평균 스코어 : 479.25\n",
            "# 에피소드수 :980, 평균 스코어 : 428.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Yx608G4GO9p1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}