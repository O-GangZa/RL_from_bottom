{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "강화학습 #6 실습.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_Idt8SNHS9un"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np  # Q러닝에서 np.argmax를 사용해서 가져옴"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GridWorld 클래스"
      ],
      "metadata": {
        "id": "JwCKmHxtYYM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GridWorld(): # 그리드월드 클래스 생성\n",
        "    def __init__(self): # 초기 변수 x, y (그리드 월드 시작 state)\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "\n",
        "    def step(self, a):\n",
        "        # 0번 액션: 왼쪽, 1번 액션: 위, 2번 액션: 오른쪽, 3번 액션 : 아래쪽\n",
        "        if a==0:\n",
        "            self.move_left() # 각 이동함수를 통해서 현재 state의 좌표를 변환한다.\n",
        "        elif a==1:\n",
        "            self.move_up()\n",
        "        elif a==2:\n",
        "            self.move_right()\n",
        "        elif a==3:\n",
        "            self.move_down()\n",
        "    \n",
        "        reward = -1 #보상은 1로 고정 (MDP를 모르므로 실제 관측하기 전까지는 모른다고 가정)\n",
        "        done = self.is_done() # done을 불리언으로 판정 terminal state 판단을 위해\n",
        "        return (self.x, self.y), reward, done\n",
        "\n",
        "    def move_left(self):\n",
        "        if self.y==0: #벽(테두리)지정\n",
        "            pass\n",
        "        elif self.y==3 and self.x in [0, 1, 2]: #장애물도 마찬가지로 지정해준다.\n",
        "            pass\n",
        "        elif self.y==5 and self.x in [2, 3, 4]:\n",
        "            pass\n",
        "        else:\n",
        "            self.y -=1\n",
        "\n",
        "    def move_right(self):\n",
        "        if self.y==1 and self.x in [0, 1, 2]:\n",
        "            pass\n",
        "        elif self.y==3 and self.x in [2, 3, 4]:\n",
        "            pass\n",
        "        elif self.y==6:\n",
        "            pass\n",
        "        else:\n",
        "            self.y +=1\n",
        "\n",
        "    def move_up(self):\n",
        "        if self.x==0:\n",
        "            pass\n",
        "        elif self.x==3 and self.y==2:\n",
        "            pass\n",
        "        else:\n",
        "            self.x -= 1\n",
        "\n",
        "    def move_down(self):\n",
        "        if self.x==4:\n",
        "            pass\n",
        "        elif self.x==1 and self.y==4:\n",
        "            pass\n",
        "        else:\n",
        "            self.x+=1\n",
        "    \n",
        "    def is_done(self):\n",
        "        if self.x==4 and self.y==6: # 목표 state인 (4, 6)에 도달하면 끝난다.\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    \n",
        "    def reset(self): #시작 state로 돌아가는 함수\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        return (self.x, self.y)"
      ],
      "metadata": {
        "id": "a4doczPXTKOa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MC QAgent 클래스"
      ],
      "metadata": {
        "id": "tPVH7iVOYdNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QAgent():\n",
        "    def __init__(self):\n",
        "        self.q_table = np.zeros((5, 7, 4)) \n",
        "        #q밸류를 저장하는 변수, 모두 0으로 초기화해준다.\n",
        "        # 상태가치함수가 아니라 행동가치함수를 찾기위해 마지막 상하좌우 행동에 따른 가치 차원을 추가해줌\n",
        "        #좌표는 (그리드의 행, 그리드의 열, 상하좌우 행동 가치)로 이루어진다\n",
        "        self.eps = 0.9 #엡실론\n",
        "        self.alpha = 0.01 #알파\n",
        "    \n",
        "    def select_action(self, s):\n",
        "        # 엡실론-그리디로 액션 선택\n",
        "        x, y = s\n",
        "        coin = random.random() # 0~1사이의 실수를 랜덤하게 뽑아줌\n",
        "        if coin < self.eps: #랜덤연산을 통해 엡실론 이하값이 나오는 경우\n",
        "            action = random.randint(0,3) #이동은 상하좌우 중 랜덤하게 선택\n",
        "        else:\n",
        "            action_val = self.q_table[x,y,:] #현재 x,y좌표의 q_table을 변수에 저장\n",
        "            action = np.argmax(action_val) #그중 제일 가치가 큰 행동을 선택해서 그쪽으로 이동함 (그리디 행동)\n",
        "        return action\n",
        "    \n",
        "    def update_table(self, history):\n",
        "        # 한 에피소드에 해당하는 history를 입력으로 받아 q 테이블의 값을 업데이트 한다.\n",
        "        cum_reward = 0   #누적 리워드 = 리턴\n",
        "        for transition in history[::-1]:  # 에피소드가 끝나면 역순으로 리턴을 체크해서\n",
        "            s, a, r , s_prime = transition\n",
        "            x, y = s  # state 확인 후\n",
        "            self.q_table[x,y,a] = self.q_table[x,y,a] + self.alpha * (cum_reward - self.q_table[x,y,a]) #해당 state(x,y 좌표에 해당하는 action의 q-table을 업데이트 해준다.\n",
        "            cum_reward = cum_reward + r\n",
        "\n",
        "    def anneal_eps(self):\n",
        "        self.eps -= 0.03 #한번 호출될때마다 입실론 값이0.03씩 내려감\n",
        "        self.eps = max(self.eps, 0.1) #최소 0.1은 보장\n",
        "    \n",
        "    def show_table(self):\n",
        "        # 학습이 각 위치에서 어느 액션의 q값이 가장 높았는지 보여주는 함수 (뭔소리?)\n",
        "        q_lst = self.q_table.tolist()  #넘파이어레이 리스트로 바꿔줌\n",
        "        data = np.zeros((5, 7))\n",
        "        for row_idx in range(len(q_lst)):\n",
        "            row = q_lst[row_idx]\n",
        "            for col_idx in range(len(row)):\n",
        "                col = row[col_idx]\n",
        "                action = np.argmax(col)\n",
        "                data[row_idx, col_idx] = action\n",
        "        print(data)"
      ],
      "metadata": {
        "id": "UGmoL0L-XYWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 메인 함수"
      ],
      "metadata": {
        "id": "bvIUHOBeyKFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    env = GridWorld()\n",
        "    agent = QAgent()\n",
        "    episode = 1000\n",
        "\n",
        "    for n_epi in range(episode): #1000번 에피소드\n",
        "        done = False\n",
        "        history = []\n",
        "\n",
        "        s = env.reset()\n",
        "        while not done: # 한 에피소드가 끝날때 까지\n",
        "            a = agent.select_action(s)\n",
        "            s_prime, r, done = env.step(a)\n",
        "            history.append((s, a, r, s_prime))\n",
        "            s = s_prime\n",
        "        agent.update_table(history) #히스토리를 이용하여 에이전트를 업데이트\n",
        "        agent.anneal_eps()\n",
        "\n",
        "    agent.show_table()\n",
        "    "
      ],
      "metadata": {
        "id": "BNB92JEEfyJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pS4g5rKRwV8U",
        "outputId": "05803507-a1b5-40e4-d937-3942e0915cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2. 3. 0. 2. 2. 3. 0.]\n",
            " [3. 3. 0. 2. 1. 3. 3.]\n",
            " [3. 3. 0. 1. 0. 3. 3.]\n",
            " [3. 2. 3. 1. 0. 2. 3.]\n",
            " [2. 1. 2. 1. 0. 2. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "각각 상하좌우 어디로 이동하는게 베스트인지를 알려주는 최적 정책 π*를 보여준다\n",
        "\n",
        "다시말해 매 그리드마다 q(s,a)가 높은 액션을 표시한 것이다."
      ],
      "metadata": {
        "id": "LEnEXkzBOPpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SARSA (MC대신 TD 사용)"
      ],
      "metadata": {
        "id": "pMvEM7a7yQuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QAgent():\n",
        "    def __init__(self):\n",
        "        self.q_table = np.zeros((5, 7, 4)) #마찬가지로 q밸류가 추가되어 3차원이다.\n",
        "        self.eps = 0.9 \n",
        "    \n",
        "    def select_action(self, s):\n",
        "        # 엡실론-그리디로 액션 선택\n",
        "        x, y = s\n",
        "        coin = random.random()\n",
        "        if coin < self.eps:\n",
        "            action = random.randint(0,3)\n",
        "        else:\n",
        "            action_val = self.q_table[x,y,:]\n",
        "            action = np.argmax(action_val)\n",
        "        return action\n",
        "    \n",
        "    # 나머지는 같고 table을 업데이트 하는 방식만 바꿔준당\n",
        "    def update_table(self, transition):\n",
        "        s, a, r , s_prime = transition  #history 대신 바로 transition을 사용 (바로 업데이트 할꺼니깐)\n",
        "        x, y = s\n",
        "        next_x, next_y = s_prime\n",
        "        a_prime = self.select_action(s_prime) # s'에서 선택할 액션을 미리 체크함\n",
        "        #SARSA 업데이트 식을 이용\n",
        "        self.q_table[x,y,a] = self.q_table[x,y,a] + 0.1*(r + self.q_table[next_x, next_y, a_prime] - self.q_table[x,y,a])\n",
        "\n",
        "    def anneal_eps(self):\n",
        "        self.eps -= 0.03 #한번 호출될때마다 0.03씩 내려감\n",
        "        self.eps = max(self.eps, 0.1) #최소 0.1은 보장\n",
        "    \n",
        "    def show_table(self):\n",
        "        # 학습이 각 위치에서 어느 액션의 q값이 가장 높았는지 보여주는 함수 (뭔소리?)\n",
        "        q_lst = self.q_table.tolist()  #넘파이어레이 리스트로 바꿔줌\n",
        "        data = np.zeros((5, 7))\n",
        "        for row_idx in range(len(q_lst)):\n",
        "            row = q_lst[row_idx]\n",
        "            for col_idx in range(len(row)):\n",
        "                col = row[col_idx]\n",
        "                action = np.argmax(col)\n",
        "                data[row_idx, col_idx] = action\n",
        "        print(data)"
      ],
      "metadata": {
        "id": "UilNoKG1wWtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    env = GridWorld()\n",
        "    agent = QAgent()\n",
        "    episode = 1000\n",
        "\n",
        "    for n_epi in range(episode): #1000번 에피소드\n",
        "        done = False\n",
        "\n",
        "        s = env.reset()\n",
        "        while not done: # 한 에피소드가 끝날때 까지\n",
        "            a = agent.select_action(s)\n",
        "            s_prime, r, done = env.step(a)\n",
        "            agent.update_table((s, a, r, s_prime))\n",
        "            s = s_prime\n",
        "        agent.anneal_eps()\n",
        "\n",
        "    agent.show_table()\n",
        "'''\n",
        "history에 리스트를 모아서 한번에 바꿔준 이전과 다르게\n",
        "매번 update_table을 호출해서 바꿔주는 것으로 TD, MC 차이를 확인가능\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FGqSumxuzooA",
        "outputId": "3edaa55f-eff2-4ecf-c798-a0e15f34b005"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nhistory에 리스트를 모아서 한번에 바꿔준 이전과 다르게\\n매번 update_table을 호출해서 바꿔주는 것으로 TD, MC 차이를 확인가능\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA_3EuLc0Zgz",
        "outputId": "620fe48f-bf4b-4786-96ea-2124ea628abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2. 3. 0. 2. 2. 2. 3.]\n",
            " [3. 3. 0. 2. 2. 3. 3.]\n",
            " [3. 3. 0. 1. 0. 3. 3.]\n",
            " [2. 2. 2. 1. 0. 3. 3.]\n",
            " [3. 2. 2. 1. 0. 2. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "수렴한 값은 비슷... 좌하단 구석에 잘못된 방향을 가리키고 있는 것을 볼수 있음\n",
        "\n",
        "\n",
        "-> 최적해는 구했지만 탐색이 부족했다."
      ],
      "metadata": {
        "id": "BzimG0QkWTqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q러닝 QAgent\n",
        "\n",
        "Class GridWorld () <- 환경이므로 건드릴 필요 없슴\n"
      ],
      "metadata": {
        "id": "6zmj2FEgFEzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QAgent():\n",
        "    def __init__(self):\n",
        "        self.q_table = np.zeros((5, 7, 4)) #q밸류를 저장하는 변수, 모두 0이고 3차원임\n",
        "        self.eps = 0.9 \n",
        "    \n",
        "    def select_action(self, s):\n",
        "        # 엡실론-그리디로 액션 선택\n",
        "        x, y = s\n",
        "        coin = random.random()\n",
        "        if coin < self.eps:\n",
        "            action = random.randint(0,3)\n",
        "        else:\n",
        "            action_val = self.q_table[x,y,:]\n",
        "            action = np.argmax(action_val)\n",
        "        return action\n",
        "\n",
        "    def update_table(self, transition):\n",
        "        s, a, r , s_prime = transition\n",
        "        x, y = s\n",
        "        next_x, next_y = s_prime\n",
        "        a_prime = self.select_action(s_prime) \n",
        "        #SARSA대신 Q러닝 업데이트 식을 이용\n",
        "        self.q_table[x,y,a] = self.q_table[x,y,a] + 0.1*(r + np.max(self.q_table[next_x, next_y, :]) - self.q_table[x,y,a])\n",
        "        #공식에 따라서 SARSA에서는 그냥 다음 액션의 Q_table을 가져오던 것에서 다음 Q_table의 max를 가져오는것으로 바뀜\n",
        "\n",
        "    def anneal_eps(self):\n",
        "        self.eps -= 0.01 #q러닝에서는 epsilon이 더 천천히 줄어들도록 함(왜?)\n",
        "        self.eps = max(self.eps, 0.2) # 0.2도 더 크다(왜?)\n",
        "\n",
        "    def show_table(self):\n",
        "        q_lst = self.q_table.tolist()  \n",
        "        data = np.zeros((5, 7))\n",
        "        for row_idx in range(len(q_lst)):\n",
        "            row = q_lst[row_idx]\n",
        "            for col_idx in range(len(row)):\n",
        "                col = row[col_idx]\n",
        "                action = np.argmax(col)\n",
        "                data[row_idx, col_idx] = action\n",
        "        print(data)"
      ],
      "metadata": {
        "id": "I1OKP22Q0cb0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6zGSmDZGrJD",
        "outputId": "ef5c1683-7f49-484d-bd33-98c84fd0fd79"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2. 3. 0. 2. 3. 3. 3.]\n",
            " [3. 3. 0. 2. 2. 3. 3.]\n",
            " [3. 3. 0. 1. 0. 3. 3.]\n",
            " [2. 2. 2. 1. 0. 2. 3.]\n",
            " [2. 1. 1. 1. 0. 2. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YyHgWS7zG8OB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}