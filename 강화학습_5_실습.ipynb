{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "강화학습 #5 실습.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random   #코인 토스를 위해서 랜덤을 import"
      ],
      "metadata": {
        "id": "Ye2r9Iiz4zAm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5-1 MC업데이트\n"
      ],
      "metadata": {
        "id": "iLGuFNSUncF6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yiiyMjLftESQ"
      },
      "outputs": [],
      "source": [
        "class GridWorld(): #그리드 월드 클래스 정의\n",
        "    def __init__(self):\n",
        "        self.x=0  # 초기 x, y 좌표를 0,0 즉 s_0로 설정해줌\n",
        "        self.y=0\n",
        "\n",
        "    def step(self, a): # 매 스텝에 따라 상하좌우로 이동한다\n",
        "        if a==0:\n",
        "            self.move_right()  \n",
        "        elif a==1:\n",
        "            self.move_left()\n",
        "        elif a==2:\n",
        "            self.move_up()\n",
        "        elif a==3:\n",
        "            self.move_down()\n",
        "        reward = -1 # 리워드는 -1로 고정 환경설정이라 지정은 해줬지만 실제로는 관측되기 전까지 모르는 값에 해당\n",
        "        done = self.is_done()  # step이 terminal state에 도달했는지 확인함\n",
        "        return (self.x, self.y), reward, done      # (state의 좌표) = (x, y)의 튜플, 리워드, done을 리턴\n",
        "\n",
        "    def move_right(self):  # 오른쪽을 선택하면 오른쪽 state로 이동\n",
        "        self.y += 1\n",
        "        if self.y > 3:    # 오른쪽벽에서 오른쪽으로 state변화가 일어난다면,\n",
        "            self.y = 3    # 제자리에 머문다 (state방문+1, 에피소드 길이도 +1, 보상도 -1 받음)\n",
        "    \n",
        "    def move_left(self):  # x, y의 이동을 제한함으로 인해 환경을 4 *4 그리드로 설정해주었다.\n",
        "        self.y -= 1\n",
        "        if self.y < 0:\n",
        "            self.y = 0\n",
        "\n",
        "    def move_up(self):\n",
        "        self.x -= 1\n",
        "        if self.x < 0:\n",
        "            self.x = 0\n",
        "\n",
        "    def move_down(self):\n",
        "        self.x += 1\n",
        "        if self.x > 3:\n",
        "            self.x = 3\n",
        "\n",
        "    def is_done(self):\n",
        "        if self.x == 3 and self.y ==3: # 좌표 3.3 (index=0)에서 시작했으므로 우하단 칸에 들어가면 terminal state로 간주\n",
        "            return True                # 불리언으로 결정 True를 반환한다.\n",
        "        else:\n",
        "            return False\n",
        "    \n",
        "    def get_state(self): # 현재 state가 뭔지 알려줌 ( = 실제로는 모르는 값에 해당, 전이확률에 따라 up,right,down,left를 입력 받더라도 다른 state를 가질 수 있다.)\n",
        "        return (self.x, self.y) \n",
        "\n",
        "    def reset(self):   # 처음 리셋을 위함\n",
        "        self.x = 0 \n",
        "        self.y = 0\n",
        "        return (self.x, self.y)    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent (): # 에이전트 클래스를 정의\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def select_action(self): # 에이전트에 포함된 정책 π\n",
        "        coin = random.random() # 각 25% 확률로(랜덤) 상,하,좌,우 를 결정해서 \n",
        "        if coin < 0.25:\n",
        "            action = 0\n",
        "        elif coin < 0.5:\n",
        "            action = 1\n",
        "        elif coin < 0.75:\n",
        "            action = 2\n",
        "        else:\n",
        "            action = 3\n",
        "        return action # 0,1,2,3으로 코딩된 action을 리턴\n"
      ],
      "metadata": {
        "id": "v58ea5OpzB-K"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    env = GridWorld() # 환경 설정\n",
        "    agent = Agent()  # 에이전트 (+ 정책 포함)\n",
        "    data = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]] # 리턴값 테이블을 만들어줌\n",
        "    gamma = 1.0 # 감쇠인자\n",
        "    alpha = 0.0001 # 업데이트 가중평균치\n",
        "\n",
        "    for k in range(50000): # 총 5만 번의 에피소드 진행\n",
        "        done = False\n",
        "        history = []\n",
        "        while not done: # terminal state가 아니면\n",
        "            action = agent.select_action()  # 랜덤(coin토스)으로 action을 결정해주고\n",
        "            (x,y), reward, done = env.step(action) # 상태변화, reward, terminal state인지 아닌지를 받음\n",
        "            history.append((x,y,reward)) # (x,y좌표, reward) 를 튜플로 append\n",
        "        env.reset()\n",
        "        cum_reward = 0   # 누적 리워드 = 리턴\n",
        "        for transition in history[::-1]:\n",
        "            # 방문했던 상태들을 뒤에서부터확인하며 차례차례 역순으로 리턴을 계산, [::-1] 리스트 인덱싱을 사용(역순 접근)\n",
        "            x, y, reward = transition\n",
        "            data[x][y] = data[x][y] + alpha*(cum_reward-data[x][y]) # x,y state에 기존 가치함수값과 가중치를 더한 새로운 리턴을 추가해줌. (1-0.0001)은 거의 1이므로 알파만 곱해준거 같다.(아마도, 연산리소스 절약을 위한거인듯)\n",
        "            cum_reward = cum_reward + gamma*reward # 리턴 계산식 = 리워드 + 감쇠인가(이후 리워드들)\n",
        "\n",
        "    # 학습이 끝나고 난 후 데이터를 출력해보는 코드\n",
        "    \n",
        "    for row in data:\n",
        "        print(row)"
      ],
      "metadata": {
        "id": "eTeeLnV5zG83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuyY-e8w1Jq0",
        "outputId": "129868cf-2b90-40eb-d330-7066b2f3a440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-60.97069528045291, -58.83812725103779, -56.576954810005375, -54.1364811519732]\n",
            "[-59.67388713683955, -55.873217943693334, -51.479086863157335, -47.074672444524495]\n",
            "[-56.68670417847033, -51.63362314455851, -42.65028397278723, -31.252977887275925]\n",
            "[-54.79325695014831, -48.20509967075206, -32.3476619387113, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.2 TD"
      ],
      "metadata": {
        "id": "i0X8UuvnnkMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    env  = GridWorld()   #가져오는 조건은 같음, MDP를 모르는 상태니 (실제로 존재하는) reward와 전이확률도 모르는 게 맞음\n",
        "    agent = Agent()  # 기본 정책도 변화없음 코인토스로 25% 상하좌우로 이동함\n",
        "    data = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]] # 리턴값 테이블을 만들어줌\n",
        "    gamma = 1.0 # 감쇠인자\n",
        "    alpha = 0.01 # 업데이트 가중평균치, MC에 비해 큰 값을 사용 (초기 변화율에는 편차가 크지만 그만큼 변화에 시간이 걸려서 인 것 같다.)\n",
        "\n",
        "    for k in range(50000): #에피소드 5만번\n",
        "        done = False # done = True로 마지막에 되어있으니 초기화\n",
        "        while not done:\n",
        "            x, y = env.get_state() #좌표 = 변화없음\n",
        "            action = agent.select_action()\n",
        "            (x_prime, y_prime), reward, done = env.step(action) #한번 step을 거침 -> (x, y)변화, reward받음, done 판단\n",
        "            x_prime, y_prime = env.get_state()\n",
        "\n",
        "            data[x][y] = data[x][y] + alpha*(reward+gamma*data[x_prime][y_prime]-data[x][y]) #(x,y) state의 상태 가치를 reward와 (x', y') 상태가치를 통해 업데이트 해줌\n",
        "        env.reset() #시작 state로 이동\n",
        "\n",
        "    # 학습 종료 후 데이터 출력\n",
        "    for row in data:\n",
        "        print(row)\n"
      ],
      "metadata": {
        "id": "bFngxZK54wrf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhSLpaTpxStF",
        "outputId": "fbdc9b8b-f79b-4e42-f329-7138de4189c0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-59.76020845652443, -57.4122112803595, -54.51269066124383, -51.82582014543628]\n",
            "[-57.646891582398766, -54.864150149525976, -50.59115967610504, -46.120652919129746]\n",
            "[-54.98804573760294, -50.2679382536611, -40.15974641266741, -29.403401000659724]\n",
            "[-51.753743375677075, -44.46147980201992, -29.526844822472704, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8VE_sz1AxVwl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}