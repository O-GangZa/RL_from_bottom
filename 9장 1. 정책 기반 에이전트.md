# 9.1 Policy Gradient



## 1. 정책 기반 에이전트는 왜 필요한가?

1. 정책 기반 에이전트는 왜 필요한가?

   - 가치 기반 에이전트의 한계 때문

     

     - 가치 기반 에이전트가 액션을 결정하는 방법  = 결정론적 Deterministic

       -> 모든 상태 s에 대해 각 상태에서 정하는 액션이 변하지 않음 

       -> 상대가 전략을 수정할 수 있다면, 바로 져버릴 것!

       

     - 반면, 정책 기반 에이전트가 액션을 결정하는 방법 = 확률적 Stochastic

       -> 어떤 상태에서 pi(a|s) 확률 분포로 action이 결정된다. 라는 식의 policy를 가질 수 있음.

       

   - 선택할 수 있는 액션이 연속적인 경우

     액션이 연속적인 값이다 = 액션을 선택하는 경우의 수가 무한이다 = 모든 값에 대한 벨류를 계산할 수 없다.

     연속적 액션공간에서는  Q(s,a)기반 에이전트가 작동하기 힘듬.

     반면, 정책기반 에이전트는 pi그래프가 주어져 있다면 바로 액션을 뽑아 줄 수 있으므로 문제가 없다.

     

   - 가치 기반 방법론에 비해 환경에 숨겨진 정보가 있거나, 환경 자체가 변하는 경우에도 더 유연하게 대처할 수 있다고 알려져 있다.



## 2. 정책 네트워크 강화에 이용되는 목적 함수 정하기

2. 정책 네트워크 강화에 이용되는 목적 함수 정하기

   pi_θ(s,a) = 정책 네트워크라고 할 때,  θ는 정책 네트워크의 파라미터                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  

   

   - 정책 기반 에이전트의 메커니즘

     1. pi_θ(s,a)로 움직이는 에이전트를 가져다 놓아 경험을 쌓고

     2. 그 경험으로 부터 pi_θ(s,a)를 강화해나가는 것 

        **즉, 하나의 에피소드가 끝난 후 학습이 진행되는 것(내 생각)**

        

   - pi_θ(s,a)를 어떻게 정의할 것인가?

     - 기존의 방법 : Gradient Descent 사용은 못함

       Loss function의 정의(MSE) : (정답 - 예측값)^2을 최소화 하는 값을 찾는 것.

       => 정책의 정답 = 최적 정책 => 최적 정책을 하면 강화학습 할 필요 X

       

     - 새로운 방법 : Gradient Ascend

       정책을 평가하는 기준을 세워서 그 값을 증가시킴 => 평가함수 = J(θ)

       평가함수를 증가 시키는 방향으로 학습한다 = pi_θ(s,a)의 값이 증가한다 = 강화학습

       

## 3. 평가함수 J(θ) 구하기

3. 평가함수 J(θ) 구하기

   정책을 어떻게 잘 평가할 것인가? 보상 합이 큰 정책이 좋은 정책!

   여기서 발생하는 문제 : **pi가 고정되어도, 에피소드 마다 서로 다른 상태를 방문하고 서로 다른 보상을 받음**

   **그렇기 때문에 기댓값 연산자가 필요하며 다음과 같이 선언할 수 있음**

   ![image-20220506083435351](../assets/img/posts/image-20220506083435351.png)

   리턴의 기댓값은... 가치함수입니다.

   -> 시작하는 상태가 S_0로 항상 고정되어 있다면 S_0의 가치라고 할 수도 있으므로 다음과 같은 식을 선언할 수 있습니다.

   ![image-20220506083625604](../assets/img/posts/image-20220506083625604.png)

   - Q1. S_0가 아닌 경우?

     시작 상태s가 S_0으로 고정인 경우 : 스타크래프트, 바둑, 체스 등

     시작 상태s가 S_0으로 고정이 아닌 경우 : 매번 다른 상태에서 시작 (예를 들어.. 시합을 했는데 부전승을 한 경우와 못한 경우?)

   - Q2. S_0의 시작이 동일하지 않으면 S_0의 가치가 왜 저 식이라고 할 수 없을까? 결국 해당 state 벨류는 앞으로 받을 보상의 합아닌가?

     (내생각) : 시합을 했는데 부전승을 한 경우면, 시합을 해야하는 횟수조차 다르므로 가치가 달라진다..? 

   

   시작 상태  s의 확률 분포를 d(s)라고 한다면 - 어떤 상태 s에서 시작할 확률을 d(s) 라고 한다면 - 다음이 성립합니다.

   ![image-20220506084227878](../assets/img/posts/image-20220506084227878.png)

   모든 상태 s에 대하여 해당 상태에서 출발 했을때 얻을 가치를 해당 상태에서 출발할 확률과 곱하여 가중합을 해준것.

   => pi_θ(s,a) 평가하는 J(θ) 함수 수립 완료



​	Q. d(s)의 뜻이 뭔가 -> 정책이터레이션이랑 연결해서 생각해보기



## 4. 평가 함수의 Gradient Ascend 구하기 

4. 평가 함수의 Gradient Ascend 구하기

   평가 함수의 업데이트 방법 :

   ![image-20220506084459242](../assets/img/posts/image-20220506084459242.png)

   => 이제 구할 것 : ▽\_θJ(θ)

   

### 4-1. 1-Step MDP에서 ▽\_θJ(θ) 구하기	

1-step MDP : 딱 한 스텝만 진행하고 바로 에피소드가 끝나는 MDP => 리턴 G\_t = 보상 reward

![image-20220506085111113](../assets/img/posts/image-20220506085111113.png)

그라디언트 적용 :

![image-20220506085146106](../assets/img/posts/image-20220506085146106.png)

(**모델-프리 인 상황이므로, R_s,a 와 Pa_ss'(전이확률)을 모름 => 위 값을 구할 수없음**) => 책에선 이렇게 나와있는데 내 생각으로는 a를 선택하는 경우의 수가 너무 많고 그에 따른 state 전이도 너무 많아서 위 식을 구할수 없다.가 더 이해하기 쉬운 표현인것 같음



**샘플 기반 방법론**을 적용할 수 있도록 식을 약간 변형하면 다음과 같다.

![image-20220506085420500](../assets/img/posts/image-20220506085420500.png)

위 형태로 바꾼 이유 = 기댓값 연산자 사용하기 위함 (기댓값 연산자가 없으면 무한의 갯수인 액션에 대한 경우를 각각 덧셈(시그마)해주어야 하는데 상태갯수가 많기에 불가능)

![image-20220506085622262](../assets/img/posts/image-20220506085622262.png)



기댓값 연산자가 붙었기 때문에 샘플 기반 방법론으로 계산 가능

pi\_θ(s,a)에 대한 기댓값이기 때문에 이제 pi\_θ(s,a)로 움직이는 Agent를 환경에 가져다 놓고 기댓값 안의 값을 여러개 모으면 됩니다.



Q1. 기댓값이 붙기전 R은 못구한다면서 왜 기댓값이 붙은 R은 구할 수 있게 되었는가? 결국에 에이전트가 실제로 움직여서 받은 보상으로 계산한다는 건데 의미가 기댓값이 붙기 전과 후에는 그 의미가 어떻게 달라지는가?

기댓값이 붙기 전 : 정책에 의해 결정된 모든 액션에 대한 보상을 계산 => 현재 상태는 MDP에 존재하는 모든 state가 큰 상황 (테이블 기반 방법론을 사용못하는 상황)이라서 모든 액션에대한 보상을 계산할 수없다.

기댓값이 붙은 후 : 기댓값이라는게 그 수가 많아지면 결국 정답에 가까워진다고 앞장 어딘가에서 나왔다. 그렇기 때문에 에피소드의 수만 충분히 많아진다면 많은 시도를 했다는 것 만으로 최적에 가까워지며, 에피소드에서 실제로 행해지는 것이기 때문에 environment가 보상을 준다. 이 값을 이용하는 것.



Q2. 위에 빨간박스가 어떻게 기댓값으로 바뀌었는가

윗 줄의 수식에 **Sigma**(pi\_θ(s,a))가 곱해진 형태이기 때문이다. **Sigma**(pi\_θ(s,a))가 곱해져 있으면 이는 그 뒤에 나올 값에 pi\_θ(s,a)만큼의 가중치를 곱해서 더해주라는 뜻이고 이게 곧 기댓값 연산자의 정의이며 이 부분이 Policy gradient의 핵심입니다.



### 4-2. 일반 MDP에서 ▽\_θJ(θ)구하기

![image-20220506090724026](../assets/img/posts/image-20220506090724026.png)

일반적인 MDP로 바뀌면 위와 같이 식이 바뀝니다.



보상 대신 s에서 a를 할 때 얻는 리턴의 기댓값으로 바뀐 것 => 한 스텝만 밟고 MDP가 종료되는 것이 아니라 이후에 받을 보상까지 더해주는 개념



Q1. 왜 V가 아니라 Q일까? 

(내생각) Policy가 존재 -> policy에 따른 action이 결정된 상황 : 그 액션의 가치를 구하는 것이라서..?



Q2. q_pi와 Q_pi는 다른건가?

같은거다.



# 9.2 REINFORCE 알고리즘 -> 노트북으로 이동

REINFORCE 알고리즘의 그라디언트 수식 : 

![image-20220506091242776](../assets/img/posts/image-20220506091242776.png)

![image-20220506091427354](../assets/img/posts/image-20220506091427354.png)

![image-20220506091535890](../assets/img/posts/image-20220506091535890.png)

Q1. G_t가 왜 편향되지 않았지? Q는 G_t의 기댓값이라 편향되지 않은 값.. 평균으로 가지만 G_t 값 하나하나는 편향된거 아닌가요?



## 1. REINFORCE pseudo code

![image-20220506091709420](../assets/img/posts/image-20220506091709420.png)



























